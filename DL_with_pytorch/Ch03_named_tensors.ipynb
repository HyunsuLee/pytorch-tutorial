{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. 텐서구조체\n",
    "## 3.4 이름이 있는 텐서\n",
    "* https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch3/2_named_tensors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3126, -0.5272, -1.0891, -0.9436,  1.3990],\n",
       "        [ 0.6383,  0.7600,  1.0159,  0.1689, -1.2332],\n",
       "        [ 0.5743,  1.6920,  1.4063,  1.0888,  1.2737],\n",
       "        [ 0.6946,  0.3385, -0.7794, -1.3830,  1.5432],\n",
       "        [ 0.1189,  1.2276,  1.8664,  1.0562, -0.1446]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[0] # by this, you can print out 5x5 image of 0th channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3126, -0.5272, -1.0891, -0.9436,  1.3990],\n",
       "         [ 0.6383,  0.7600,  1.0159,  0.1689, -1.2332],\n",
       "         [ 0.5743,  1.6920,  1.4063,  1.0888,  1.2737],\n",
       "         [ 0.6946,  0.3385, -0.7794, -1.3830,  1.5432],\n",
       "         [ 0.1189,  1.2276,  1.8664,  1.0562, -0.1446]],\n",
       "\n",
       "        [[ 0.8139,  0.4725,  2.1944,  0.1213, -0.1462],\n",
       "         [-0.3711,  1.2576,  0.1531, -1.3440,  0.4629],\n",
       "         [-1.0107,  0.4668, -0.0521, -0.2063,  0.2032],\n",
       "         [-0.1380, -1.4370,  0.6503, -0.9988, -0.6896],\n",
       "         [-0.1973,  0.2925, -0.1569, -0.7541,  0.4995]],\n",
       "\n",
       "        [[-0.7742, -0.2897,  1.5014, -1.0714,  1.5222],\n",
       "         [-1.3452,  0.6464,  0.0677, -0.7798, -0.0619],\n",
       "         [-0.9003,  0.4632,  0.9690,  0.1639, -0.4950],\n",
       "         [ 1.1165,  0.8820,  0.0460,  0.1322, -0.0371],\n",
       "         [ 1.3960,  0.6710,  1.2203,  0.1145,  0.5692]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[0:] # print out all tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2091,  0.1995, -0.7416,  0.1909, -0.4584],\n",
       "         [ 1.7072, -0.5648, -0.7092,  0.3115,  0.6508],\n",
       "         [ 0.8624,  1.2366,  2.0773, -1.2187,  0.5553],\n",
       "         [ 1.1659, -2.5488,  0.6690, -0.9867, -0.2997],\n",
       "         [-1.5916,  0.5156,  0.5894, -0.8936, -0.3151]],\n",
       "\n",
       "        [[ 0.4081,  0.6684, -1.3292,  0.1385, -0.9094],\n",
       "         [ 0.0332,  0.6434, -0.3412, -0.7759, -1.0176],\n",
       "         [ 2.5028, -0.4970, -1.0738,  0.7211, -0.7651],\n",
       "         [-0.6766,  2.1116, -0.2088, -0.6841,  0.2709],\n",
       "         [ 0.0701, -0.8069,  0.0925,  0.3426,  2.0478]],\n",
       "\n",
       "        [[-2.0998, -0.2465,  0.7039, -0.1692, -0.2314],\n",
       "         [-0.0746,  1.0015, -0.3049, -1.4522, -1.3886],\n",
       "         [-0.6563,  0.9668,  0.8859,  3.1799, -0.5477],\n",
       "         [ 0.6126,  0.1032, -1.0765, -1.1260, -0.5255],\n",
       "         [-0.0160, -2.4074,  0.7244,  0.6569, -0.2978]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t[0] # first dim of tensor is batch, thus by this you print out first image data with 3x5x5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 5])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[None].size() # if you add [none], you add one dim at 0th. \n",
    "# imagine you wrap whole data with one dimension of tensor again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3) # mean through channel dimenstion, thus disappear channel\n",
    "batch_gray_naive = batch_t.mean(-3) # mean through channel dimenstion, thus disappear channel\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights)\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2126, 0.7152, 0.0722]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights.unsqueeze(0))\n",
    "weights.unsqueeze(0).size() # unsqueeze(0) is equivalent to [none]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2126],\n",
      "        [0.7152],\n",
      "        [0.0722]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights.unsqueeze(-1))\n",
    "weights.unsqueeze(-1).size() # unsqueeze(-1) add dimension with one length at last dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size() # but unsqueeze method do not change original tensor, but unsqueeze_ do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1) \n",
    "# original code use unsqueeze_ but it doesn't affect because wegiths.unsqueezed(-1) is temp tensor.\n",
    "# thus weights.unsqueeze(-1).unsqueeze_(-1) do no effect to original tensor. \n",
    "# if you want change original tensor write in weights.unsqueeze_(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights.size(), img_t.size(), unsqueezed_weights.size()\n",
    "# img_weights = (img_t * unsqueezed_weights) dose broadcast. \n",
    "# 3x1x1 tensor stech out to 3x5x5 and pointwise multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights.shape, batch_t.shape # this calculation was pivoted channel dimension, it has same length.\n",
    "# rest dimension was , I think, broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3) # sum through changel dimension\n",
    "img_gray_weighted.size(), batch_gray_weighted.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "# above code is equivalent with unsqueezing weights and broadcast, \n",
    "# einsum method give name to each dimenstion of tensor, '...' means broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5927,  0.2049,  1.4463, -0.1912,  0.3028],\n",
       "        [-0.2269,  1.1077,  0.3303, -0.9817,  0.0644],\n",
       "        [-0.6657,  0.7270,  0.3317,  0.0958,  0.3804],\n",
       "        [ 0.1296, -0.8921,  0.3027, -0.9988, -0.1678],\n",
       "        [-0.0150,  0.5186,  0.3727, -0.3065,  0.3676]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5927,  0.2049,  1.4463, -0.1912,  0.3028],\n",
       "        [-0.2269,  1.1077,  0.3303, -0.9817,  0.0644],\n",
       "        [-0.6657,  0.7270,  0.3317,  0.0958,  0.3804],\n",
       "        [ 0.1296, -0.8921,  0.3027, -0.9988, -0.1678],\n",
       "        [-0.0150,  0.5186,  0.3727, -0.3065,  0.3676]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named =  img_t.refine_names(..., 'channels', 'rows', 'columns') # you can redefine name of tensor's dim\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns') # \"...\" mean ignoring preceded dim\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named) # align_as method give same\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서끼리 연산 할 때는 차원의 크기가 같은지, 아니면 적어도 브로드캐스팅 될 수 있는 확인해야 한다. \n",
    "* 만약 이름이 지정되어 있지만, pytorch가 체크한다. aling_as는 빠진 차원을 채워준다.\n",
    "* 위 코드에서 weights_named는 channel이라는 dimension만 가진 1차원 tensor였는데, align_as로 채워졌다. \n",
    "* 즉 차원을 비교하며 unsqueeze를 할 필요없다는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels') # arg can be the name of dimension\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum('channels')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can remove the name of dimensions\n",
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 책에서는 앞으로 텐서 이름을 지정하지 않는다고함. (왜?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 dtype 관련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype=torch.double)  # <1>\n",
    "points_short = points_64.to(torch.short) # short dtype is signed integer\n",
    "points_64 * points_short  # works from PyTorch 1.3 onwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 텐서 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.transpose(0, 1) # equivalent with above cell\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.t() # only work with 2D tensor\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 텐서 저장소(storage)관점에서 보기, 3.8 텐서 메타데이터: 사이즈, 오프셋, 스트라이드\n",
    "* 이건 ../DL_torch_book/Ch01_intro.ipynb 내용과 겹쳐 생략."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.8.3 transpose of high dimesion tensor\n",
    "some_t = torch.ones(3, 4, 5)\n",
    "transpose_t = some_t.transpose(0, 2) # swith(transpose) 0index dim and 2index dim\n",
    "some_t.shape, transpose_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 5, 1), (1, 5, 20))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.stride(), transpose_t.stride() # dimenstion with stride 1 = contigouos tensor\n",
    "# it is effectively approach memory in point of view of data locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]), (12, 3, 1))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t_con = transpose_t.contiguous() # to make this tensor as coniguous\n",
    "# it change form of storage()\n",
    "transpose_t_con.shape, transpose_t_con.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('th1p12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540ee3e445d1da392d2b8a7d647dcbc2bd268a0d669d2281aeea3f4a5da62e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
