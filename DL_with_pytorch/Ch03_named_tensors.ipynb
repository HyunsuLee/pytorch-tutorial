{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. 텐서구조체\n",
    "## 3.4 이름이 있는 텐서\n",
    "* https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch3/2_named_tensors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226815/1646494204.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1408.)\n",
      "  _ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7383,  1.9188, -0.2760, -0.2143,  1.8746],\n",
       "        [-0.8046, -0.7873, -1.2865, -1.1809,  0.1765],\n",
       "        [-1.3356,  1.9046,  2.0137,  0.4127,  0.4568],\n",
       "        [ 0.2562, -1.2760,  1.9799, -1.8147,  2.0187],\n",
       "        [-0.1531,  2.2671,  1.2157,  1.0379, -0.3470]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[0] # by this, you can print out 5x5 image of 0th channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7383,  1.9188, -0.2760, -0.2143,  1.8746],\n",
       "         [-0.8046, -0.7873, -1.2865, -1.1809,  0.1765],\n",
       "         [-1.3356,  1.9046,  2.0137,  0.4127,  0.4568],\n",
       "         [ 0.2562, -1.2760,  1.9799, -1.8147,  2.0187],\n",
       "         [-0.1531,  2.2671,  1.2157,  1.0379, -0.3470]],\n",
       "\n",
       "        [[ 0.0618, -0.2955,  0.1364,  0.3346,  0.2909],\n",
       "         [ 0.6658,  0.0674, -0.8405, -2.3237,  1.2399],\n",
       "         [-1.0121, -0.5547,  0.5389, -2.8539, -2.3026],\n",
       "         [ 0.3248,  1.2078, -0.8312, -0.2602,  1.5023],\n",
       "         [-0.9522,  1.7975, -0.2441,  0.0856,  0.6456]],\n",
       "\n",
       "        [[-0.0656,  0.2458,  0.0124, -0.0262,  1.2666],\n",
       "         [ 0.1017, -0.5298,  0.6226,  0.5870,  0.3674],\n",
       "         [ 0.5646,  1.2552,  0.5984, -0.6512,  0.3381],\n",
       "         [-0.6264, -0.0401, -0.6419, -0.7837,  0.1894],\n",
       "         [ 0.1878, -0.4564, -0.2905,  1.3058, -1.4142]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[0:] # print out all tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.2818e-01, -6.2737e-01, -1.1961e+00,  5.3530e-01, -1.0213e+00],\n",
       "         [-1.7466e+00,  1.4068e+00, -1.9954e-01,  6.1419e-01, -7.8994e-01],\n",
       "         [-4.4506e-01,  6.1148e-01,  3.8056e-01,  1.7084e-01,  9.1795e-03],\n",
       "         [ 3.0943e-01, -1.6437e+00,  9.4434e-01,  3.8656e-02,  5.0007e-01],\n",
       "         [ 5.8637e-01,  1.1931e-01,  2.0387e-01,  1.6190e+00,  1.5756e-01]],\n",
       "\n",
       "        [[-1.6395e+00, -1.8684e+00,  1.0098e+00,  1.6866e+00,  3.5682e-01],\n",
       "         [-1.5056e+00, -1.8162e+00,  8.7935e-01,  1.9987e+00,  2.4348e+00],\n",
       "         [ 3.1926e-01,  7.4567e-01, -3.4235e-01, -4.7339e-01, -4.4813e-02],\n",
       "         [ 4.8869e-02, -9.8366e-01, -5.3018e-02, -1.8790e-03,  6.4851e-02],\n",
       "         [-2.8480e-01, -5.7439e-01, -6.2209e-01,  2.6701e-02,  8.7711e-02]],\n",
       "\n",
       "        [[ 1.8136e+00, -1.6606e-01, -1.9662e+00, -3.9675e-01,  1.1401e-01],\n",
       "         [ 2.8895e-01, -3.4719e-01, -1.0204e+00, -8.4504e-01,  6.7931e-01],\n",
       "         [-1.0789e-01,  5.0379e-01,  9.7689e-01,  1.8768e+00,  1.0116e+00],\n",
       "         [ 1.1947e+00, -1.0770e+00,  9.9776e-01, -7.7628e-03,  1.8114e+00],\n",
       "         [-1.1423e+00,  2.9807e-01,  6.2810e-01,  9.4203e-01, -1.1368e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t[0] # first dim of tensor is batch, thus by this you print out first image data with 3x5x5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t[None].size() # if you add [none], you add one dim at 0th. \n",
    "# imagine you wrap whole data with one dimension of tensor again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3) # mean through channel dimenstion, thus disappear channel\n",
    "batch_gray_naive = batch_t.mean(-3) # mean through channel dimenstion, thus disappear channel\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights)\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2126, 0.7152, 0.0722]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights.unsqueeze(0))\n",
    "weights.unsqueeze(0).size() # unsqueeze(0) is equivalent to [none]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2126],\n",
      "        [0.7152],\n",
      "        [0.0722]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights.unsqueeze(-1))\n",
    "weights.unsqueeze(-1).size() # unsqueeze(-1) add dimension with one length at last dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size() # but unsqueeze method do not change original tensor, but unsqueeze_ do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1) \n",
    "# original code use unsqueeze_ but it doesn't affect because wegiths.unsqueezed(-1) is temp tensor.\n",
    "# thus weights.unsqueeze(-1).unsqueeze_(-1) do no effect to original tensor. \n",
    "# if you want change original tensor write in weights.unsqueeze_(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights.size(), img_t.size(), unsqueezed_weights.size()\n",
    "# img_weights = (img_t * unsqueezed_weights) dose broadcast. \n",
    "# 3x1x1 tensor stech out to 3x5x5 and pointwise multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights.shape, batch_t.shape # this calculation was pivoted channel dimension, it has same length.\n",
    "# rest dimension was , I think, broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3) # sum through changel dimension\n",
    "img_gray_weighted.size(), batch_gray_weighted.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "# above code is equivalent with unsqueezing weights and broadcast, \n",
    "# einsum method give name to each dimenstion of tensor, '...' means broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1964,  0.2143,  0.0398,  0.1918,  0.6980],\n",
       "        [ 0.3125, -0.1575, -0.8297, -1.8706,  0.9508],\n",
       "        [-0.9671,  0.0988,  0.8568, -2.0003, -1.5253],\n",
       "        [ 0.2416,  0.5896, -0.2199, -0.6285,  1.5173],\n",
       "        [-0.7000,  1.7346,  0.0629,  0.3762,  0.2858]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1964,  0.2143,  0.0398,  0.1918,  0.6980],\n",
       "        [ 0.3125, -0.1575, -0.8297, -1.8706,  0.9508],\n",
       "        [-0.9671,  0.0988,  0.8568, -2.0003, -1.5253],\n",
       "        [ 0.2416,  0.5896, -0.2199, -0.6285,  1.5173],\n",
       "        [-0.7000,  1.7346,  0.0629,  0.3762,  0.2858]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named =  img_t.refine_names(..., 'channels', 'rows', 'columns') # you can redefine name of tensor's dim\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns') # \"...\" mean ignoring preceded dim\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named) # align_as method give same\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서끼리 연산 할 때는 차원의 크기가 같은지, 아니면 적어도 브로드캐스팅 될 수 있는 확인해야 한다. \n",
    "* 만약 이름이 지정되어 있지만, pytorch가 체크한다. aling_as는 빠진 차원을 채워준다.\n",
    "* 위 코드에서 weights_named는 channel이라는 dimension만 가진 1차원 tensor였는데, align_as로 채워졌다. \n",
    "* 즉 차원을 비교하며 unsqueeze를 할 필요없다는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels') # arg can be the name of dimension\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum('channels')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can remove the name of dimensions\n",
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 책에서는 앞으로 텐서 이름을 지정하지 않는다고함. (왜?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 dtype 관련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype=torch.double)  # <1>\n",
    "points_short = points_64.to(torch.short) # short dtype is signed integer\n",
    "points_64 * points_short  # works from PyTorch 1.3 onwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 텐서 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.transpose(0, 1) # equivalent with above cell\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.t() # only work with 2D tensor\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 텐서 저장소(storage)관점에서 보기, 3.8 텐서 메타데이터: 사이즈, 오프셋, 스트라이드\n",
    "* 이건 ../DL_torch_book/Ch01_intro.ipynb 내용과 겹쳐 생략."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([5, 4, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.8.3 transpose of high dimesion tensor\n",
    "some_t = torch.ones(3, 4, 5)\n",
    "transpose_t = some_t.transpose(0, 2) # swith(transpose) 0index dim and 2index dim\n",
    "some_t.shape, transpose_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 5, 1), (1, 5, 20))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.stride(), transpose_t.stride() # dimenstion with stride 1 = contigouos tensor\n",
    "# it is effectively approach memory in point of view of data locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]), (12, 3, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t_con = transpose_t.contiguous() # to make this tensor as coniguous\n",
    "# it change form of storage()\n",
    "transpose_t_con.shape, transpose_t_con.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 텐서를 GPU에서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using device argument\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_gpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64_gpu = points_64.to(device=device)\n",
    "points_64.device, points_64_gpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = 2 * points_64 # calculated on CPU\n",
    "points_64_gpu = 2 * points_64.to(device=device) # calculated on GPU\n",
    "points_64_gpu.device # and remain the result in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64_gpu = points_64.cuda(0) #another method, int arg can set the specific GPU\n",
    "points_64_gpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_64_cpu = points_64_gpu.cpu()\n",
    "point_64_cpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.int16, torch.float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64_gpu = points_64.to(device=device, dtype=torch.short) # move the data and simultaneously change to dtype\n",
    "points_64_gpu.device, points_64_gpu.dtype, points_64.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 numpy 호환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.ones(3,4)\n",
    "points_np = points.numpy()\n",
    "points_np # note that dtype is float32, contrast to that default dtype of numpy is float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.from_numpy(points_np)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_gpu = points.cuda()\n",
    "#points_np = points_gpu.numpy() # directly convert to numpy is not working, if tensor is on GPU\n",
    "# you have to move the tensor back to cpu to convert to numpy\n",
    "points_cpu = points_gpu.cpu()\n",
    "points_np = points_cpu.numpy()\n",
    "points_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.12 텐서 직렬화, 저장, 호환되는 형식으로 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ch03/ourpoints.t', 'wb') as f:\n",
    "    torch.save(points, f) # torch use pickle in backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/ch03/ourpoints.t', 'rb') as f:\n",
    "    points_loaded= torch.load(f)\n",
    "points_loaded\n",
    "# but this type is only working for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('./data/ch03/ourpoints.hdf5', 'w')\n",
    "dset = f.create_dataset('coord', data=points.numpy())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hdf5 do not have to open whole data on the disk, it can directly index data on the disk\n",
    "f = h5py.File('./data/ch03/ourpoints.hdf5', 'r')\n",
    "dset = f['coord']\n",
    "last_points = dset[-2:]\n",
    "last_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_points = torch.from_numpy(dset[-2:])\n",
    "f.close()\n",
    "last_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you cann't approach to dset object after f.close()\n"
     ]
    }
   ],
   "source": [
    "# if closed, you cann't approach to dset object\n",
    "try:\n",
    "    dset[-2:]\n",
    "except:\n",
    "    print(\" you cann't approach to dset object after f.close()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./data/ch03/ourpoints.hdf5', 'r') as f: #equivalent with open()\n",
    "    dset = f['coord']\n",
    "    last_points = dset[-2:]\n",
    "print(last_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 연습문제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9]), (1,), 0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "ex1 = list(range(9))\n",
    "ex1_tensor = torch.tensor(ex1)\n",
    "ex1_tensor.size(), ex1_tensor.stride(), ex1_tensor.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.a\n",
    "#print(a)\n",
    "b = a.view(2,3)\n",
    "a_t = a.t()\n",
    "a, b, a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (3, 1), (1, 2))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.stride(), b.stride(), a_t.stride() # view act like tranpose + contigous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2]), 4, (3, 1), tensor([[1., 1.]]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.c\n",
    "c = b[1:,1:]\n",
    "c.size(), c.storage_offset(), c.stride(), c # sub_tensor maintain same stride with original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4269, 0.4678],\n",
       "        [0.0624, 0.6665]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.rand(2,2)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('th1p12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540ee3e445d1da392d2b8a7d647dcbc2bd268a0d669d2281aeea3f4a5da62e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
